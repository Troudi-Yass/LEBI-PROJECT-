{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a56afa",
   "metadata": {},
   "source": [
    "## Phase 1 ‚Äì Website Structure Analysis\n",
    "\n",
    "An initial analysis was conducted to determine how job offers are loaded on the\n",
    "Hellowork platform in order to select an appropriate data extraction method.\n",
    "\n",
    "Hellowork provides continuously updated job listings, advanced search filters,\n",
    "sector-based navigation, and personalized features for users. These functionalities\n",
    "require real-time interaction with backend systems and dynamic content generation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Hellowork is identified as a **dynamic website**, where job offers are rendered\n",
    "dynamically rather than being fully embedded in static HTML pages. Consequently,\n",
    "the data extraction process will be implemented using **Selenium**, which allows\n",
    "the execution of client-side scripts and reliable access to dynamic content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7dfae",
   "metadata": {},
   "source": [
    "## Phase 1 ‚Äì Scraping Scope Definition\n",
    "\n",
    "### Search Criteria\n",
    "\n",
    "To build a dataset suitable for large-scale business intelligence analysis and machine learning tasks, the scraping process targets a broad and representative set of job offers published on the Hellowork platform.\n",
    "\n",
    "The search criteria are intentionally defined in a **general and cross-domain manner**, covering multiple professional fields, including but not limited to:\n",
    "- Information Technology and digital professions\n",
    "- Engineering and technical roles\n",
    "- Business, management, and administration\n",
    "- Marketing, communication, and sales\n",
    "- Finance, accounting, and economics\n",
    "- Human resources and support functions\n",
    "\n",
    "The data collection relies on generic job search result pages without restrictive filters on job category, experience level, or contract type. This approach allows the aggregation of a diverse sample of job offers reflecting various sectors, seniority levels, and employment types.\n",
    "\n",
    "To ensure sufficient data volume for clustering, classification, and trend analysis, the scraping scope is designed to collect **at least 2,000 job offers**. This is achieved by iterating over multiple search result pages and aggregating offers across different categories and search queries.\n",
    "\n",
    "Only currently available job offers are considered, and location information is preserved to support geographic analysis in later phases of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c620327",
   "metadata": {},
   "source": [
    "## Phase 1 ‚Äì Data Fields Definition\n",
    "\n",
    "To ensure a structured and complete dataset, the following fields will be extracted from each job offer on **Hellowork**:\n",
    "\n",
    "| Field | Description | Notes / Purpose |\n",
    "|-------|------------|----------------|\n",
    "| `Sector` | Name of the job sector | Tracks the category/industry of the job; useful for filtering and analytics |\n",
    "| `Job_Title` | The title of the job position | Key feature for clustering, keyword extraction, and dashboard display |\n",
    "| `Company` | Name of the hiring company | Useful for analytics, filtering, and company-specific trends |\n",
    "| `Location` | City or region of the job | Enables geographic analysis in dashboards |\n",
    "| `Contract` | Type of employment contract (CDI, CDD, Internship, etc.) | Categorical feature for ML and filtering |\n",
    "| `Salary` | Salary as displayed in the job posting | Will be cleaned and standardized during ETL |\n",
    "| `Description` | Full textual description of the position | Main input for NLP, clustering, and keyword extraction |\n",
    "| `URL` | Direct link to the job offer | Reference for validation, scraping completeness, and linking |\n",
    "| `additional_info` | Optional field for other relevant details (e.g., benefits, remote work) | Can capture extra structured or unstructured info; may be empty |\n",
    "\n",
    "### Notes\n",
    "- Some fields may be **missing** in certain job postings (e.g., salary or contract). Missing values will be handled during ETL.  \n",
    "- Text fields (`Job_Title`, `Description`) will be **preprocessed** for NLP tasks in Phase 3.  \n",
    "- Categorical fields (`Sector`, `Contract`, `Location`) will be **encoded** during preprocessing.  \n",
    "- The dataset will be **saved incrementally** in `hellowork_progress.csv` and finalized as `hellowork_final_sectors_data.csv`.  \n",
    "\n",
    "This structured field definition ensures that the dataset is **consistent, ML-ready, and suitable for visualization** in later phases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf3090",
   "metadata": {},
   "source": [
    "## Phase 1 ‚Äì HTML Structure Analysis\n",
    "\n",
    "To accurately extract the defined data fields, the HTML structure of Hellowork job listings was analyzed using browser developer tools. This step identifies the **HTML tags, CSS selectors, and containers** corresponding to each field.\n",
    "\n",
    "### Mapping of Data Fields to HTML Elements\n",
    "\n",
    "| Field | HTML Tag / CSS Selector | Notes |\n",
    "|-------|------------------------|-------|\n",
    "| `Job_Title` | `[data-cy=\"jobTitle\"]` (usually inside `<h1>` or `<a>`) | Main title of the job; used as the primary identifier for the job offer |\n",
    "| `Company` | `h1 a` | Name of the hiring company; displayed near the job title |\n",
    "| `Location` | `ul.tw-flex.tw-flex-wrap.tw-gap-3 li:nth-child(1)` | City/region of the job; first `<li>` under job info list |\n",
    "| `Contract` | `ul.tw-flex.tw-flex-wrap.tw-gap-3 li:nth-child(2)` | Employment type (CDI, CDD, Internship); second `<li>` under job info list |\n",
    "| `Salary` | `[data-cy=\"salary-tag-button\"]` | Optional; may be missing for some offers |\n",
    "| `Description` | `[data-truncate-text-target=\"content\"]` | Full textual job description; line breaks are removed in preprocessing |\n",
    "| `URL` | `href` attribute of the `<a>` tag linking to job offer | Direct link to job details; used for navigation and validation |\n",
    "| `Sector` | N/A (from sector list in scraping loop) | Assigned based on the sector being scraped |\n",
    "| `additional_info` | Optional; not explicitly extracted in current code | Can be added later if extra details are needed |\n",
    "\n",
    "### Notes\n",
    "- Some job postings may **omit optional fields** (e.g., `Salary` or `Contract`). Missing values are handled in ETL.  \n",
    "- All fields are **available in the loaded HTML** after page load; no dynamic JS rendering is required for the current selectors.  \n",
    "- Using this mapping, the scraping script can reliably extract each field for all job offers across sectors.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c723e0a",
   "metadata": {},
   "source": [
    "## Phase 1 ‚Äì Scraping Workflow and Strategy\n",
    "\n",
    "With the HTML structure and data fields defined, the scraping workflow is designed to extract all job offers efficiently and reliably while respecting ethical standards.\n",
    "\n",
    "### 1. Workflow Overview\n",
    "\n",
    "1. **Set up Selenium WebDriver** (Chrome) with proper options:\n",
    "   - User-agent header\n",
    "   - Maximized window\n",
    "   - Optional headless mode\n",
    "2. **Access the search results page** for each sector.\n",
    "3. **Handle cookies banners** and other pop-ups to ensure uninterrupted scraping.\n",
    "4. **Iterate over each job listing on the page**:\n",
    "   - Extract job URL from the search results\n",
    "   - Open job detail page to scrape all defined data fields (`Job_Title`, `Company`, `Location`, `Contract`, `Salary`, `Description`, `URL`, `Sector`)\n",
    "   - Use **explicit waits** to ensure elements are loaded before extraction\n",
    "5. **Store extracted data** in a structured format (CSV)\n",
    "6. **Repeat for all pages** of each sector until the target of ‚â• 2,000 job offers is reached\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Pagination Handling\n",
    "\n",
    "- Each search result page contains multiple job listings (typically 20‚Äì25 per page)  \n",
    "- The scraper navigates through pages by **modifying the page number parameter** in the URL (`&p=page_number`)  \n",
    "- Iteration continues until:\n",
    "  - Enough job offers are collected (‚â• 2,000)  \n",
    "  - No more pages are available or the page contains no job listings\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Error Handling\n",
    "\n",
    "- **Missing fields**: optional fields (Salary, Contract) are stored as `\"N/A\"` or `None`  \n",
    "- **Timeouts / page load issues**: use Selenium **explicit waits** and retry logic for failed elements  \n",
    "- **Unexpected page structure**: log the issue and continue with remaining jobs  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Politeness and Ethics\n",
    "\n",
    "- Add a **delay of 0.5‚Äì3 seconds** between requests and job detail visits to avoid overloading the server  \n",
    "- Respect the website‚Äôs **robots.txt** and terms of use  \n",
    "- Scraping is performed **only for academic purposes**, not for commercial use  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data Storage\n",
    "\n",
    "- Extracted data is **saved incrementally** to `hellowork_progress.csv` to prevent loss in case of interruption  \n",
    "- Final dataset saved as `hellowork_final_sectors_data.csv`  \n",
    "- CSV encoding: UTF-8 to preserve special characters  \n",
    "- Data includes all defined fields and sector information, ready for **Phase 2 ‚Äì ETL and Data Cleaning**\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Summary\n",
    "\n",
    "This workflow ensures:\n",
    "- Reliable extraction of ‚â• 2,000 job offers across multiple sectors  \n",
    "- Structured and consistent dataset with all required fields  \n",
    "- Ethical, reproducible, and robust scraping process using Selenium  \n",
    "- Preparedness for subsequent **ETL, ML, and interactive dashboard** phases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97795968",
   "metadata": {},
   "source": [
    "## Libraries Used in Phase 1 ‚Äì Scraping\n",
    "\n",
    "- `time`  \n",
    "  Provides functions to **pause execution** (e.g., `sleep`) between requests to avoid overloading the server.\n",
    "\n",
    "- `pandas` (`pd`)  \n",
    "  Used for **data manipulation and storage**. Allows saving scraped job offers to CSV (`.to_csv`) and performing ETL in later phases.\n",
    "\n",
    "- `selenium`  \n",
    "  Automates **browser interactions** for scraping dynamic websites. Main components used:  \n",
    "  - `webdriver` ‚Äì controls the browser (Chrome in this case)  \n",
    "  - `Service` ‚Äì manages the ChromeDriver service  \n",
    "  - `By` ‚Äì locates HTML elements by ID, CSS selector, XPath, etc.  \n",
    "  - `WebDriverWait` ‚Äì explicit waits until elements are present or clickable  \n",
    "  - `expected_conditions` (`EC`) ‚Äì conditions for waits, e.g., element visibility or clickability  \n",
    "  - `TimeoutException` ‚Äì handles cases when a wait exceeds the maximum time  \n",
    "\n",
    "- `webdriver_manager.chrome` (`ChromeDriverManager`)  \n",
    "  Automatically **downloads and manages the correct ChromeDriver version**, simplifying Selenium setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50362de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Import Required Libraries ===\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6808e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Configuration & Sector List ===\n",
    "\n",
    "BASE_SEARCH_URL = \"https://www.hellowork.com/fr-fr/emploi/recherche.html?k=job+%C3%A9tudiant&st=relevance\"\n",
    "MAX_PAGES_PER_SECTOR = 10\n",
    "\n",
    "SECTORS_LIST = [\n",
    "    {\"id\": \"Agri_peche\", \"name\": \"Agriculture ‚Ä¢ P√™che\"},\n",
    "    {\"id\": \"BTP\", \"name\": \"BTP\"},\n",
    "    {\"id\": \"Banq_assur_finan\", \"name\": \"Banque ‚Ä¢ Assurance ‚Ä¢ Finance\"},\n",
    "    {\"id\": \"Distrib_commerce\", \"name\": \"Distribution ‚Ä¢ Commerce de gros\"},\n",
    "    {\"id\": \"Enseign_forma\", \"name\": \"Enseignement ‚Ä¢ Formation\"},\n",
    "    {\"id\": \"Immo\", \"name\": \"Immobilier\"},\n",
    "    {\"id\": \"Ind_agro\", \"name\": \"Industrie Agro ‚Ä¢ alimentaire\"},\n",
    "    {\"id\": \"Ind_auto_meca_nav\", \"name\": \"Industrie Auto ‚Ä¢ Meca ‚Ä¢ Navale\"},\n",
    "    {\"id\": \"Ind_aero\", \"name\": \"Industrie A√©ronautique ‚Ä¢ A√©rospatial\"},\n",
    "    {\"id\": \"Ind_manufact\", \"name\": \"Industrie Manufacturi√®re\"},\n",
    "    {\"id\": \"Ind_pharma_bio_chim\", \"name\": \"Industrie Pharmaceutique ‚Ä¢ Biotechn. ‚Ä¢ Chimie\"},\n",
    "    {\"id\": \"Ind_petro\", \"name\": \"Industrie P√©troli√®re ‚Ä¢ P√©trochimie\"},\n",
    "    {\"id\": \"Ind_hightech_telecom\", \"name\": \"Industrie high ‚Ä¢ tech ‚Ä¢ Telecom\"},\n",
    "    {\"id\": \"Media_internet_com\", \"name\": \"M√©dia ‚Ä¢ Internet ‚Ä¢ Communication\"},\n",
    "    {\"id\": \"Resto\", \"name\": \"Restauration\"},\n",
    "    {\"id\": \"Sante_social\", \"name\": \"Sant√© ‚Ä¢ Social ‚Ä¢ Association\"},\n",
    "    {\"id\": \"Energie_envir\", \"name\": \"Secteur Energie ‚Ä¢ Environnement\"},\n",
    "    {\"id\": \"Inform_SSII\", \"name\": \"Secteur informatique ‚Ä¢ ESN\"},\n",
    "    {\"id\": \"Serv_public_autre\", \"name\": \"Service public autres\"},\n",
    "    {\"id\": \"Serv_public_etat\", \"name\": \"Service public d'√©tat\"},\n",
    "    {\"id\": \"Serv_public_collec_terri\", \"name\": \"Service public des collectivit√©s territoriales\"},\n",
    "    {\"id\": \"Serv_public_hosp\", \"name\": \"Service public hospitalier\"},\n",
    "    {\"id\": \"Serv_entreprise\", \"name\": \"Services aux Entreprises\"},\n",
    "    {\"id\": \"Serv_pers_part\", \"name\": \"Services aux Personnes ‚Ä¢ Particuliers\"},\n",
    "    {\"id\": \"Tourism_hotel_loisir\", \"name\": \"Tourisme ‚Ä¢ H√¥tellerie ‚Ä¢ Loisirs\"},\n",
    "    {\"id\": \"Transport_logist\", \"name\": \"Transport ‚Ä¢ Logistique\"}\n",
    "]\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "print(f\"‚úì Configuration loaded: {len(SECTORS_LIST)} sectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40626de1",
   "metadata": {},
   "source": [
    "## Step 3: Helper Functions for WebDriver Management\n",
    "\n",
    "These functions manage the Selenium WebDriver and handle common tasks like cookies and navigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Setup WebDriver\n",
    "def setup_driver():\n",
    "    \"\"\"Setup Chrome WebDriver with appropriate options.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Uncomment for background execution\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(f\"user-agent={USER_AGENT}\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    print(\"‚úì WebDriver initialized\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "# Function 2: Handle Cookies Banner\n",
    "def handle_cookies(driver):\n",
    "    \"\"\"Close cookies banner if present.\"\"\"\n",
    "    try:\n",
    "        cookie_btn = WebDriverWait(driver, 4).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"hw-cc-notice-continue-without-accepting-btn\"))\n",
    "        )\n",
    "        cookie_btn.click()\n",
    "        print(\"‚úì Cookies handled.\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"‚úì No cookies banner found.\")\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ce939",
   "metadata": {},
   "source": [
    "## Step 4: Job Detail Extraction Function\n",
    "\n",
    "This function extracts all relevant information from each job posting page using Selenium selectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_details(driver, url, sector_name):\n",
    "    \"\"\"Scrape details from a specific job offer page.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        url: Job offer URL\n",
    "        sector_name: Name of the sector\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with job details\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    data = {\n",
    "        \"Sector\": sector_name,\n",
    "        \"Job_Title\": \"N/A\",\n",
    "        \"Company\": \"N/A\",\n",
    "        \"Location\": \"N/A\",\n",
    "        \"Contract\": \"N/A\",\n",
    "        \"Salary\": \"N/A\",\n",
    "        \"Description\": \"N/A\",\n",
    "        \"Publication_Date\": \"N/A\",\n",
    "        \"URL\": url\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "\n",
    "        # 1. Job Title\n",
    "        try:\n",
    "            data[\"Job_Title\"] = driver.find_element(By.CSS_SELECTOR, '[data-cy=\"jobTitle\"]').text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 2. Company\n",
    "        try:\n",
    "            data[\"Company\"] = driver.find_element(By.CSS_SELECTOR, 'h1 a').text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 3. Location & Contract\n",
    "        try:\n",
    "            tags = driver.find_elements(By.CSS_SELECTOR, 'ul.tw-flex.tw-flex-wrap.tw-gap-3 li')\n",
    "            if len(tags) > 0: data[\"Location\"] = tags[0].text.strip()\n",
    "            if len(tags) > 1: data[\"Contract\"] = tags[1].text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 4. Salary\n",
    "        try:\n",
    "            data[\"Salary\"] = driver.find_element(By.CSS_SELECTOR, '[data-cy=\"salary-tag-button\"]').text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 5. Description\n",
    "        try:\n",
    "            desc = driver.find_element(By.CSS_SELECTOR, '[data-truncate-text-target=\"content\"]').text\n",
    "            data[\"Description\"] = desc.replace(\"\\n\", \" \").strip()\n",
    "        except: pass\n",
    "\n",
    "        # 6. Publication Date\n",
    "        try:\n",
    "            date_elem = driver.find_element(By.CSS_SELECTOR, '.tw-typo-xs.tw-text-grey-500')\n",
    "            date_text = date_elem.text.strip()\n",
    "            if date_text:\n",
    "                data[\"Publication_Date\"] = date_text\n",
    "        except: pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting details for {url}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "print(\"‚úì scrape_job_details() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797457c",
   "metadata": {},
   "source": [
    "## Step 5: Main Scraping Loop - Execute Full Pipeline\n",
    "\n",
    "This is the main function that orchestrates the entire scraping process:\n",
    "1. Iterates through all 26 sectors\n",
    "2. For each sector, scrapes multiple pages\n",
    "3. For each job listing, extracts complete details\n",
    "4. Saves progress incrementally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main scraping function - iterates through all sectors and collects job data.\"\"\"\n",
    "    driver = setup_driver()\n",
    "    all_results = []\n",
    "\n",
    "    try:\n",
    "        driver.get(BASE_SEARCH_URL)\n",
    "        handle_cookies(driver)\n",
    "\n",
    "        for sector in SECTORS_LIST:\n",
    "            s_name = sector['name']\n",
    "            s_id = sector['id']\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"START SECTOR: {s_name} (ID: {s_id})\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            for page in range(1, MAX_PAGES_PER_SECTOR + 1):\n",
    "                sector_url = f\"{BASE_SEARCH_URL}&s={s_id}&p={page}\"\n",
    "                print(f\"Page {page} | Sector URL: {sector_url}\")\n",
    "                driver.get(sector_url)\n",
    "\n",
    "                try:\n",
    "                    WebDriverWait(driver, 6).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'a[data-cy=\"offerTitle\"]'))\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    print(f\"‚ö† No jobs on page {page}, skipping rest of sector.\")\n",
    "                    break\n",
    "\n",
    "                offer_elems = driver.find_elements(By.CSS_SELECTOR, 'a[data-cy=\"offerTitle\"]')\n",
    "                urls_to_visit = list(set([elem.get_attribute(\"href\") for elem in offer_elems]))\n",
    "                print(f\"‚úì Found {len(urls_to_visit)} unique jobs on this page.\")\n",
    "\n",
    "                for url in urls_to_visit:\n",
    "                    job_data = scrape_job_details(driver, url, s_name)\n",
    "                    all_results.append(job_data)\n",
    "                    time.sleep(0.5)  # Politeness delay\n",
    "\n",
    "            # Save progress after each sector\n",
    "            progress_file = \"hellowork_progress.csv\"\n",
    "            pd.DataFrame(all_results).to_csv(progress_file, index=False, encoding='utf-8')\n",
    "            print(f\"üìä Progress saved: {len(all_results)} jobs so far\")\n",
    "\n",
    "    finally:\n",
    "        # Final save\n",
    "        if all_results:\n",
    "            df = pd.DataFrame(all_results)\n",
    "            final_filename = \"hellowork_final_sectors_data.csv\"\n",
    "            df.to_csv(final_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"‚úì SCRAPING COMPLETED\")\n",
    "            print(f\"File: {final_filename}\")\n",
    "            print(f\"Total jobs: {len(all_results)}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# === EXECUTE SCRAPING ===\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
