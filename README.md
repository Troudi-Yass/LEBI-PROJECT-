# LEBI Project â€“ Job Offers Analysis Pipeline

## ğŸ¯ Overview

The **LEBI Project** is a complete end-to-end data pipeline for scraping, cleaning, enriching, and visualizing job offers from HelloWork. It demonstrates professional data engineering and machine learning workflows.

### ğŸ”„ Pipeline Phases

1. **Phase 1: Web Scraping** â€“ Extract job data from HelloWork
2. **Phase 2: ETL** â€“ Clean, standardize, and normalize data
3. **Phase 3: Machine Learning** â€“ Cluster jobs and predict salary categories
4. **Phase 4: Dashboard** â€“ Interactive visualization with Dash

---

## ğŸ“ Project Structure

```
LEBI PROJECT/
â”œâ”€â”€ data/                          # Data directory (gitignored)
â”‚   â”œâ”€â”€ raw/                       # Phase 1 output: raw scraped data
â”‚   â”œâ”€â”€ processed/                 # Phase 2 output: cleaned data
â”‚   â””â”€â”€ enriched/                  # Phase 3 output: ML-enriched data
â”‚
â”œâ”€â”€ src/                           # Source code (modular architecture)
â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â””â”€â”€ hellowork_scraper.py   # Web scraper
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â””â”€â”€ data_cleaning.py       # ETL pipeline
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ vectorization.py       # TF-IDF vectorization
â”‚   â”‚   â”œâ”€â”€ clustering.py          # KMeans clustering
â”‚   â”‚   â””â”€â”€ classification.py      # Salary prediction
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â””â”€â”€ app.py                 # Dash web application
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py              # Configuration & logging
â”‚
â”œâ”€â”€ run_scraping.py                # Run Phase 1
â”œâ”€â”€ run_etl.py                     # Run Phase 2
â”œâ”€â”€ run_ml.py                      # Run Phase 3
â”œâ”€â”€ run_dashboard.py               # Run Phase 4
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate it (Windows)
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Pipeline

**Option A: Run All Phases Sequentially**

```bash
# Phase 1: Scraping (optional if data already exists)
python run_scraping.py

# Phase 2: ETL (required)
python run_etl.py

# Phase 3: Machine Learning (required)
python run_ml.py

# Phase 4: Dashboard (view results)
python run_dashboard.py
```

**Option B: Skip Scraping (Use Existing Data)**

If you already have `hellowork_final_sectors_data.csv` in the `data/raw/` folder:

```bash
python run_etl.py
python run_ml.py
python run_dashboard.py
```

### 3. View Dashboard

Open your browser to: **http://127.0.0.1:8050/**

---

## ğŸ“Š Data Flow

```
Raw CSV (data/raw/)
    â†“
[Phase 2: ETL]
    â†“
Cleaned CSV (data/processed/)
    â†“
[Phase 3: ML]
    â†“
Enriched CSV (data/enriched/)
    â†“
[Phase 4: Dashboard]
    â†“
Interactive Visualizations
```

---

## ğŸ› ï¸ Key Features

### Phase 1: Web Scraping
- Requests + BeautifulSoup for fast parsing
- Selenium fallback for JavaScript-heavy pages
- Robust error handling and logging

### Phase 2: ETL
- Duplicate removal
- Salary normalization (handles ranges, units, hourly/yearly)
- Missing data handling
- TF-IDF keyword extraction
- Categorical encoding

### Phase 3: Machine Learning
- **Clustering**: KMeans with auto K-selection (silhouette score)
- **Classification**: Logistic Regression for salary prediction
- AUC scores and evaluation metrics

### Phase 4: Dashboard
- Interactive filters (sector, location, contract type, cluster, salary)
- Real-time visualizations:
  - Job distribution by sector
  - Salary distribution histogram
  - Cluster scatter plot
  - Top companies bar chart

---

## ğŸ“¦ Module Documentation

### `src.utils.config`
Central configuration hub with:
- File paths (RAW_CSV, CLEAN_CSV, ENRICHED_CSV)
- Logging factory (`get_logger()`)
- Directory management (`ensure_dirs()`)

### `src.scraping.hellowork_scraper`
Web scraper with:
- `scrape_listings()` â€“ Main scraping function
- `fetch_page()` â€“ HTTP/Selenium page fetching
- Dynamic content detection

### `src.etl.data_cleaning`
ETL pipeline with:
- `load_raw()` â€“ Load and standardize columns
- `clean_duplicates()` â€“ Remove duplicates
- `normalize_salary()` â€“ Parse salary strings
- `prepare_clean()` â€“ Full pipeline execution

### `src.ml.vectorization`
- `build_tfidf_matrix()` â€“ Create TF-IDF vectors from text

### `src.ml.clustering`
- `find_optimal_k()` â€“ Auto-select K using silhouette score
- `apply_kmeans()` â€“ Cluster jobs by description similarity

### `src.ml.classification`
- `prepare_labels()` â€“ Create binary salary labels
- `train_logistic()` â€“ Train classifier and return metrics

### `src.dashboard.app`
- `create_app()` â€“ Build Dash layout and callbacks
- `run()` â€“ Launch server

---

## ğŸ“ Educational Notes

> **âš ï¸ Note PÃ©dagogique:** This scraping was done for educational purposes only, without intensive automation or data resale. The project demonstrates ETL, ML, and visualization concepts in an educational context.

---

## ğŸ“ Requirements

- Python 3.8+
- See `requirements.txt` for full dependencies

---

## ğŸ› Troubleshooting

**Issue: "Raw CSV not found"**
- Ensure `hellowork_final_sectors_data.csv` exists in `data/raw/`
- Or run `python run_scraping.py` to generate it

**Issue: "Module not found"**
- Activate virtual environment: `.venv\Scripts\activate`
- Install dependencies: `pip install -r requirements.txt`

**Issue: Dashboard won't start**
- Ensure Phase 2 and 3 completed successfully
- Check that enriched CSV exists in `data/enriched/`

---

## ğŸ‘¥ Contributors

LEBI Project Team

---

## ğŸ“„ License

Educational Project - Not for commercial use
