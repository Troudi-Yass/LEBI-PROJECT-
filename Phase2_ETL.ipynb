{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf75f74f",
   "metadata": {},
   "source": [
    "# **Phase 2 ‚Äì Data Preparation (ETL)**\n",
    "\n",
    "## Overview\n",
    "Phase 2 focuses on **cleaning, transforming, and structuring** the raw job offer data collected in Phase 1.\n",
    "\n",
    "We'll:\n",
    "1. **Load** the raw CSV from Phase 1\n",
    "2. **Clean** duplicates and handle missing values\n",
    "3. **Standardize** salary formats to monthly EUR values\n",
    "4. **Parse** French relative dates (aujourd'hui, hier, il y a X jours)\n",
    "5. **Process** job descriptions with NLTK (remove stopwords, punctuation)\n",
    "6. **Extract** keywords using TF-IDF vectorization\n",
    "7. **Encode** categorical variables (sectors, locations, contract types)\n",
    "8. **Save** the cleaned dataset for Phase 3 (ML)\n",
    "\n",
    "### **Phase Outcome**\n",
    "‚úÖ Clean, structured, ML-ready dataset saved to `hellowork_cleaned.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a948e2",
   "metadata": {},
   "source": [
    "## **Step 1 ‚Äì Import Libraries**\n",
    "\n",
    "Below are all the libraries we need for ETL:\n",
    "\n",
    "- **pandas/numpy**: Load, clean, and manipulate datasets\n",
    "- **re, string**: Text pattern matching and punctuation removal  \n",
    "- **nltk**: French text preprocessing (stopwords, tokenization)\n",
    "- **sklearn.feature_extraction**: TF-IDF vectorization for text\n",
    "- **sklearn.preprocessing**: Encode categorical variables (OneHot, Label)\n",
    "- **datetime**: Parse and manipulate publication dates\n",
    "- **Path, os**: File system operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddffd51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# NLP and Text Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup French stopwords\n",
    "FRENCH_STOPWORDS = set(stopwords.words('french'))\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49699646",
   "metadata": {},
   "source": [
    "## **Step 2 ‚Äì Load Raw Dataset**\n",
    "\n",
    "Load the final scraped CSV from Phase 1 (`hellowork_final_sectors_data.csv`).\n",
    "\n",
    "**Objectives:**\n",
    "- Read the CSV file with UTF-8 encoding\n",
    "- Display shape, columns, and preview data\n",
    "- Understand the raw data structure before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7e268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset shape: (1374, 9)\n",
      "\n",
      "üìã Columns:\n",
      "['Sector', 'Job_Title', 'Company', 'Location', 'Contract', 'Salary', 'Description', 'Publication_Date', 'URL']\n",
      "\n",
      "üîç Data types:\n",
      "Sector               object\n",
      "Job_Title            object\n",
      "Company              object\n",
      "Location             object\n",
      "Contract             object\n",
      "Salary               object\n",
      "Description          object\n",
      "Publication_Date    float64\n",
      "URL                  object\n",
      "dtype: object\n",
      "\n",
      "üìà First 3 rows:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Contract</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Description</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agriculture ‚Ä¢ P√™che</td>\n",
       "      <td>Alternance - Charg√©¬∑e de Formation H/F</td>\n",
       "      <td>Remy Cointreau</td>\n",
       "      <td>Paris - 75</td>\n",
       "      <td>Alternance</td>\n",
       "      <td>486,49 - 1‚ÄØ801,80 ‚Ç¨ / mois</td>\n",
       "      <td>Nous recherchons un¬∑e candidat¬∑e :  Alternance...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/642118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTP</td>\n",
       "      <td>Alternance-Gestionnaire Paie H/F</td>\n",
       "      <td>Lafarge France</td>\n",
       "      <td>Issy-les-Moulineaux - 92</td>\n",
       "      <td>Alternance</td>\n",
       "      <td>486,49 - 1‚ÄØ801,80 ‚Ç¨ / mois</td>\n",
       "      <td>Pourquoi nous rejoindre ?  &gt; Participer √† la t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/729761...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BTP</td>\n",
       "      <td>Ouvrier Polyvalent en Menuiserie H/F</td>\n",
       "      <td>Groupe Actual</td>\n",
       "      <td>Auterive - 31</td>\n",
       "      <td>Int√©rim</td>\n",
       "      <td>Estimation ‚Üí 12,36 - 13,50 ‚Ç¨ / heure</td>\n",
       "      <td>Nous recherchons un(e) menuisier(e) exp√©riment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/735245...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Sector                               Job_Title  \\\n",
       "0  Agriculture ‚Ä¢ P√™che  Alternance - Charg√©¬∑e de Formation H/F   \n",
       "1                  BTP        Alternance-Gestionnaire Paie H/F   \n",
       "2                  BTP    Ouvrier Polyvalent en Menuiserie H/F   \n",
       "\n",
       "          Company                  Location    Contract  \\\n",
       "0  Remy Cointreau                Paris - 75  Alternance   \n",
       "1  Lafarge France  Issy-les-Moulineaux - 92  Alternance   \n",
       "2   Groupe Actual             Auterive - 31     Int√©rim   \n",
       "\n",
       "                                 Salary  \\\n",
       "0            486,49 - 1‚ÄØ801,80 ‚Ç¨ / mois   \n",
       "1            486,49 - 1‚ÄØ801,80 ‚Ç¨ / mois   \n",
       "2  Estimation ‚Üí 12,36 - 13,50 ‚Ç¨ / heure   \n",
       "\n",
       "                                         Description  Publication_Date  \\\n",
       "0  Nous recherchons un¬∑e candidat¬∑e :  Alternance...               NaN   \n",
       "1  Pourquoi nous rejoindre ?  > Participer √† la t...               NaN   \n",
       "2  Nous recherchons un(e) menuisier(e) exp√©riment...               NaN   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://www.hellowork.com/fr-fr/emplois/642118...  \n",
       "1  https://www.hellowork.com/fr-fr/emplois/729761...  \n",
       "2  https://www.hellowork.com/fr-fr/emplois/735245...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 2: Load Raw Dataset ---\n",
    "raw_path = \"data/raw/hellowork_final_sectors_data.csv\"\n",
    "\n",
    "df = pd.read_csv(raw_path, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"\\nüìã Columns:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nüîç Data types:\\n{df.dtypes}\")\n",
    "print(f\"\\nüìà First 3 rows:\\n\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5b93d",
   "metadata": {},
   "source": [
    "## **Step 3 ‚Äì Clean Duplicates & Handle Missing Values**\n",
    "\n",
    "Before processing, we need to:\n",
    "1. **Remove duplicate rows** (same job posted multiple times)\n",
    "2. **Identify missing values** in key columns\n",
    "3. **Fill text columns** with `\"Not specified\"` where empty\n",
    "4. **Verify data consistency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542b1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Removed duplicates: 1374 ‚Üí 1219 rows\n",
      "   Job_Title: filled 0 missing values\n",
      "   Company: filled 15 missing values\n",
      "   Location: filled 0 missing values\n",
      "   Contract: filled 0 missing values\n",
      "   Description: filled 0 missing values\n",
      "\n",
      "‚úÖ Missing values after cleaning:\n",
      "Sector                 0\n",
      "Job_Title              0\n",
      "Company                0\n",
      "Location               0\n",
      "Contract               0\n",
      "Salary                 0\n",
      "Description            0\n",
      "Publication_Date    1219\n",
      "URL                    0\n",
      "dtype: int64\n",
      "\n",
      "üìä Current shape: (1219, 9)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Clean Duplicates & Handle Missing Values ---\n",
    "\n",
    "# Remove exact duplicates\n",
    "before_clean = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"üóëÔ∏è  Removed duplicates: {before_clean} ‚Üí {len(df)} rows\")\n",
    "\n",
    "# Handle missing values in text columns\n",
    "text_columns = [\"Job_Title\", \"Company\", \"Location\", \"Contract\", \"Description\"]\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        df[col] = df[col].fillna(\"Not specified\")\n",
    "        print(f\"   {col}: filled {missing_count} missing values\")\n",
    "\n",
    "# Show missing values after cleaning\n",
    "print(f\"\\n‚úÖ Missing values after cleaning:\")\n",
    "print(df.isna().sum())\n",
    "print(f\"\\nüìä Current shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d719f0",
   "metadata": {},
   "source": [
    "## **Step 4 ‚Äì Parse French Relative Dates**\n",
    "\n",
    "Job postings use French relative dates like:\n",
    "- `\"aujourd'hui\"` (today)\n",
    "- `\"hier\"` (yesterday)  \n",
    "- `\"il y a 3 jours\"` (3 days ago)\n",
    "- `\"il y a 2 mois\"` (2 months ago)\n",
    "\n",
    "We need to **convert these to actual timestamps** for analysis and temporal trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed8b25",
   "metadata": {},
   "source": [
    "# --- Step 4: Parse French Relative Dates ---\n",
    "\n",
    "def parse_relative_date(date_str):\n",
    "    \"\"\"Convert French relative dates to timestamps.\n",
    "    \n",
    "    Examples:\n",
    "    - \"aujourd'hui\" ‚Üí today's date\n",
    "    - \"hier\" ‚Üí yesterday's date\n",
    "    - \"il y a 3 jours\" ‚Üí 3 days ago\n",
    "    - \"il y a 2 mois\" ‚Üí ~60 days ago\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or date_str == \"Not specified\":\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).lower()\n",
    "    today = datetime.today().date()\n",
    "    \n",
    "    try:\n",
    "        if \"hier\" in date_str:\n",
    "            return pd.Timestamp(today - timedelta(days=1))\n",
    "        elif \"aujourd'hui\" in date_str or \"today\" in date_str:\n",
    "            return pd.Timestamp(today)\n",
    "        elif \"il y a\" in date_str:\n",
    "            # Extract the number (e.g., \"il y a 3 jours\" ‚Üí 3)\n",
    "            nums = re.findall(r'\\d+', date_str)\n",
    "            if nums:\n",
    "                val = int(nums[0])\n",
    "                if \"mois\" in date_str:\n",
    "                    return pd.Timestamp(today - timedelta(days=val*30))\n",
    "                elif \"jour\" in date_str:\n",
    "                    return pd.Timestamp(today - timedelta(days=val))\n",
    "                elif \"heure\" in date_str or \"minute\" in date_str:\n",
    "                    return pd.Timestamp(today)  # Same day\n",
    "        # Fallback: try standard date parsing\n",
    "        return pd.to_datetime(date_str, dayfirst=True, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "# Apply date parsing\n",
    "if \"Publication_Date\" in df.columns:\n",
    "    df[\"Publication_Date\"] = df[\"Publication_Date\"].apply(parse_relative_date)\n",
    "    print(f\"üìÖ Date range: {df['Publication_Date'].min()} to {df['Publication_Date'].max()}\")\n",
    "    print(f\"   Missing dates: {df['Publication_Date'].isna().sum()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'Publication_Date' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbe5ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 9) (3604834790.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mWe'll **extract numeric values** and **convert to monthly EUR**, handling ranges by averaging.\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 9)\n"
     ]
    }
   ],
   "source": [
    "## **Step 5 ‚Äì Normalize Salary Fields**\n",
    "\n",
    "Salaries come in **many formats**:\n",
    "- `\"30k‚Ç¨\"`, `\"30 000 ‚Ç¨\"` (monthly or annual?)\n",
    "- `\"2500 ‚Ç¨/mois\"` (monthly)\n",
    "- `\"36000 ‚Ç¨/an\"` (annual)\n",
    "- `\"2500-3000\"` (range)\n",
    "\n",
    "We'll **extract numeric values** and **convert to monthly EUR**, handling ranges by averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85c7215",
   "metadata": {},
   "source": [
    "# --- Step 5: Normalize Salary Fields ---\n",
    "\n",
    "def normalize_salary(value):\n",
    "    \"\"\"Convert salary strings to monthly numeric value (EUR).\n",
    "    \n",
    "    Handles:\n",
    "    - Ranges: \"2500-3000\" ‚Üí 2750 (average)\n",
    "    - Annual: \"36000 ‚Ç¨/an\" ‚Üí 3000 (monthly)\n",
    "    - Hourly: \"15 ‚Ç¨/h\" ‚Üí 2400 (assuming 160h/month)\n",
    "    - Missing: returns NaN\n",
    "    \"\"\"\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return np.nan\n",
    "    \n",
    "    s = value.lower()\n",
    "    # Clean whitespace variants\n",
    "    s = s.replace(\"\\u202f\", \" \").replace(\"\\xa0\", \" \").replace(\"\\u2009\", \" \")\n",
    "    s = s.replace(\"√† partir de\", \"\").replace(\"estimation\", \"\").strip()\n",
    "    \n",
    "    # Detect units\n",
    "    per_hour = \"heure\" in s or \"/heure\" in s or \"‚Ç¨/h\" in s\n",
    "    per_year = \"an\" in s or \"/an\" in s or \"annuel\" in s\n",
    "    per_month = \"mois\" in s or \"/mois\" in s or \"mensuel\" in s\n",
    "    \n",
    "    # Extract all numbers\n",
    "    num_matches = re.findall(r\"\\d+[\\d\\.\\s]*[\\,\\.]?\\d*\", s)\n",
    "    clean_nums = []\n",
    "    for m in num_matches:\n",
    "        m_clean = m.strip().replace(\" \", \"\").replace(\"\\u202f\", \"\")\n",
    "        if m_clean.count(',') > 0 and m_clean.count('.') == 0:\n",
    "            m_clean = m_clean.replace(',', '.')\n",
    "        if m_clean.count('.') > 1:\n",
    "            m_clean = m_clean.replace('.', '')\n",
    "        m_clean = re.sub(r\"[^0-9\\.]\", \"\", m_clean)\n",
    "        try:\n",
    "            clean_nums.append(float(m_clean))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not clean_nums:\n",
    "        return np.nan\n",
    "    \n",
    "    # Average if range\n",
    "    val = float(sum(clean_nums[:2]) / len(clean_nums[:2])) if len(clean_nums) > 1 else float(clean_nums[0])\n",
    "    \n",
    "    # Detect 'k' multiplier\n",
    "    if 'k' in s:\n",
    "        val = val * 1000\n",
    "    \n",
    "    # Convert to monthly\n",
    "    if per_hour:\n",
    "        monthly = val * 160  # 160 working hours/month\n",
    "    elif per_year:\n",
    "        monthly = val / 12\n",
    "    elif per_month:\n",
    "        monthly = val\n",
    "    else:\n",
    "        # Ambiguous: assume annual if > 5000, else monthly\n",
    "        monthly = val / 12 if val > 5000 else val\n",
    "    \n",
    "    return float(monthly)\n",
    "\n",
    "\n",
    "# Apply salary normalization\n",
    "if \"Salary\" in df.columns:\n",
    "    df[\"Salary_Monthly\"] = df[\"Salary\"].astype(str).apply(normalize_salary)\n",
    "    print(f\"üí∞ Salary statistics (monthly EUR):\")\n",
    "    print(df[\"Salary_Monthly\"].describe())\n",
    "    print(f\"\\n   Missing salaries: {df['Salary_Monthly'].isna().sum()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'Salary' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425905c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 5) (2822500190.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m- Common French words (stopwords) that don't add meaning\u001b[39m\n                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 5)\n"
     ]
    }
   ],
   "source": [
    "## **Step 6 ‚Äì Clean Text with NLTK**\n",
    "\n",
    "Job descriptions contain:\n",
    "- Punctuation, line breaks, excessive whitespace\n",
    "- Common French words (stopwords) that don't add meaning\n",
    "- Mixed case and special characters\n",
    "\n",
    "We'll **preprocess text** for ML:\n",
    "1. Convert to lowercase\n",
    "2. Remove punctuation and line breaks\n",
    "3. **Remove French stopwords** (le, la, de, et, etc.)\n",
    "4. This creates clean, tokenized descriptions ready for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c1297",
   "metadata": {},
   "source": [
    "# --- Step 6: Clean Text with NLTK ---\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, remove punctuation, and drop French stopwords.\"\"\"\n",
    "    if pd.isna(text) or text == \"Not specified\":\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [word for word in text.split() if word not in FRENCH_STOPWORDS]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning to descriptions\n",
    "if \"Description\" in df.columns:\n",
    "    df[\"Description_Clean\"] = df[\"Description\"].apply(clean_text)\n",
    "    print(\"üßπ Cleaned text sample:\")\n",
    "    print(df[[\"Description\", \"Description_Clean\"]].head(3))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'Description' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c542c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (768220705.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mWe use **TF-IDF vectorization** to surface the top keywords from each description.\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## **Step 7 ‚Äì Extract Keywords with TF-IDF**\n",
    "\n",
    "We use **TF-IDF vectorization** to surface the top keywords from each description.\n",
    "\n",
    "**Why TF-IDF?** It weights words by importance: common words get low weight, rare informative words get higher weight.\n",
    "\n",
    "**Goal:** Add a `Top_Keywords` column with the top terms per job description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fb3f6",
   "metadata": {},
   "source": [
    "## **Step 8 ‚Äì Encode Categorical Variables**\n",
    "\n",
    "To make categories ML-friendly, we **factorize** columns like `Sector`, `Location`, `Contract`, `Company` into numeric codes.\n",
    "\n",
    "This keeps the dataset lightweight and ready for clustering/classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2638fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Sample keywords:\n",
      "                                         Description  \\\n",
      "0  Nous recherchons un¬∑e candidat¬∑e :  Alternance...   \n",
      "1  Pourquoi nous rejoindre ?  > Participer √† la t...   \n",
      "2  Nous recherchons un(e) menuisier(e) exp√©riment...   \n",
      "\n",
      "                                        Top_Keywords  \n",
      "0  formation,formations,des,de,groupe,aider,cr√©at...  \n",
      "1             paie,de,et,la,des,groupe,processus,ses  \n",
      "2  recherchons,ayant,un,nous,avons,connaissance,c...  \n"
     ]
    }
   ],
   "source": [
    "# --- Step 7: Extract Keywords with TF-IDF ---\n",
    "\n",
    "def extract_keywords_tfidf(texts, top_k=10):\n",
    "    \"\"\"Return top-k keywords for each document using TF-IDF.\"\"\"\n",
    "    vect = TfidfVectorizer(max_features=1000, stop_words=None)\n",
    "    X = vect.fit_transform(texts)\n",
    "    feature_names = vect.get_feature_names_out()\n",
    "    \n",
    "    def top_terms(row):\n",
    "        if row.nnz == 0:\n",
    "            return \"\"\n",
    "        scores = zip(row.indices, row.data)\n",
    "        sorted_terms = sorted(scores, key=lambda x: -x[1])[:top_k]\n",
    "        return \",\".join(feature_names[i] for i, _ in sorted_terms)\n",
    "\n",
    "    return [top_terms(X[i]) for i in range(X.shape[0])]\n",
    "\n",
    "\n",
    "text_col = \"Description_Clean\" if \"Description_Clean\" in df.columns else \"Description\"\n",
    "texts = df[text_col].fillna(\"\").astype(str).tolist()\n",
    "df[\"Top_Keywords\"] = extract_keywords_tfidf(texts, top_k=8)\n",
    "\n",
    "print(\"üîë Sample keywords:\")\n",
    "print(df[[text_col, \"Top_Keywords\"]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f929f",
   "metadata": {},
   "source": [
    "## **Step 9 ‚Äì Save Cleaned Dataset**\n",
    "\n",
    "Finally, we save the cleaned dataset for Phase 3 modeling.\n",
    "\n",
    "**Output:** `data/processed/hellowork_cleaned.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3d6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è Encoded Sector ‚Üí Sector_enc\n",
      "üè∑Ô∏è Encoded Location ‚Üí Location_enc\n",
      "üè∑Ô∏è Encoded Contract ‚Üí Contract_enc\n",
      "üè∑Ô∏è Encoded Company ‚Üí Company_enc\n",
      "\n",
      "üìä Encoded columns added: ['Sector', 'Location', 'Contract', 'Company']\n",
      "Current shape: (1219, 14)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 8: Encode Categorical Variables ---\n",
    "\n",
    "categorical_cols = [c for c in [\"Sector\", \"Location\", \"Contract\", \"Company\"] if c in df.columns]\n",
    "for c in categorical_cols:\n",
    "    df[c + \"_enc\"] = pd.factorize(df[c].astype(str))[0]\n",
    "    print(f\"üè∑Ô∏è Encoded {c} ‚Üí {c+'_enc'}\")\n",
    "\n",
    "print(f\"\\nüìä Encoded columns added: {categorical_cols}\")\n",
    "print(f\"Current shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac348947",
   "metadata": {},
   "source": [
    "# --- Step 9: Save Cleaned Dataset ---\n",
    "\n",
    "output_path = \"data/processed/hellowork_cleaned.csv\"\n",
    "Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"üíæ Saved cleaned dataset to {output_path} with shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e91e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved cleaned dataset to data/processed/hellowork_cleaned.csv with shape (1219, 14)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Step 9: Save Cleaned Dataset ---\n",
    "\n",
    "output_path = \"data/processed/hellowork_cleaned.csv\"\n",
    "Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"üíæ Saved cleaned dataset to {output_path} with shape {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
