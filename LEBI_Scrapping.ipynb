{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a56afa",
   "metadata": {},
   "source": [
    "## Phase 1 – Website Structure Analysis\n",
    "\n",
    "An initial analysis was conducted to determine how job offers are loaded on the\n",
    "Hellowork platform in order to select an appropriate data extraction method.\n",
    "\n",
    "Hellowork provides continuously updated job listings, advanced search filters,\n",
    "sector-based navigation, and personalized features for users. These functionalities\n",
    "require real-time interaction with backend systems and dynamic content generation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Hellowork is identified as a **dynamic website**, where job offers are rendered\n",
    "dynamically rather than being fully embedded in static HTML pages. Consequently,\n",
    "the data extraction process will be implemented using **Selenium**, which allows\n",
    "the execution of client-side scripts and reliable access to dynamic content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7dfae",
   "metadata": {},
   "source": [
    "## Phase 1 – Scraping Scope Definition\n",
    "\n",
    "### Search Criteria\n",
    "\n",
    "To build a dataset suitable for large-scale business intelligence analysis and machine learning tasks, the scraping process targets a broad and representative set of job offers published on the Hellowork platform.\n",
    "\n",
    "The search criteria are intentionally defined in a **general and cross-domain manner**, covering multiple professional fields, including but not limited to:\n",
    "- Information Technology and digital professions\n",
    "- Engineering and technical roles\n",
    "- Business, management, and administration\n",
    "- Marketing, communication, and sales\n",
    "- Finance, accounting, and economics\n",
    "- Human resources and support functions\n",
    "\n",
    "The data collection relies on generic job search result pages without restrictive filters on job category, experience level, or contract type. This approach allows the aggregation of a diverse sample of job offers reflecting various sectors, seniority levels, and employment types.\n",
    "\n",
    "To ensure sufficient data volume for clustering, classification, and trend analysis, the scraping scope is designed to collect **at least 2,000 job offers**. This is achieved by iterating over multiple search result pages and aggregating offers across different categories and search queries.\n",
    "\n",
    "Only currently available job offers are considered, and location information is preserved to support geographic analysis in later phases of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c620327",
   "metadata": {},
   "source": [
    "## Phase 1 – Data Fields Definition\n",
    "\n",
    "To ensure a structured and complete dataset, the following fields will be extracted from each job offer on **Hellowork**:\n",
    "\n",
    "| Field | Description | Notes / Purpose |\n",
    "|-------|------------|----------------|\n",
    "| `Sector` | Name of the job sector | Tracks the category/industry of the job; useful for filtering and analytics |\n",
    "| `Job_Title` | The title of the job position | Key feature for clustering, keyword extraction, and dashboard display |\n",
    "| `Company` | Name of the hiring company | Useful for analytics, filtering, and company-specific trends |\n",
    "| `Location` | City or region of the job | Enables geographic analysis in dashboards |\n",
    "| `Contract` | Type of employment contract (CDI, CDD, Internship, etc.) | Categorical feature for ML and filtering |\n",
    "| `Salary` | Salary as displayed in the job posting | Will be cleaned and standardized during ETL |\n",
    "| `Description` | Full textual description of the position | Main input for NLP, clustering, and keyword extraction |\n",
    "| `URL` | Direct link to the job offer | Reference for validation, scraping completeness, and linking |\n",
    "| `additional_info` | Optional field for other relevant details (e.g., benefits, remote work) | Can capture extra structured or unstructured info; may be empty |\n",
    "\n",
    "### Notes\n",
    "- Some fields may be **missing** in certain job postings (e.g., salary or contract). Missing values will be handled during ETL.  \n",
    "- Text fields (`Job_Title`, `Description`) will be **preprocessed** for NLP tasks in Phase 3.  \n",
    "- Categorical fields (`Sector`, `Contract`, `Location`) will be **encoded** during preprocessing.  \n",
    "- The dataset will be **saved incrementally** in `hellowork_progress.csv` and finalized as `hellowork_final_sectors_data.csv`.  \n",
    "\n",
    "This structured field definition ensures that the dataset is **consistent, ML-ready, and suitable for visualization** in later phases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf3090",
   "metadata": {},
   "source": [
    "## Phase 1 – HTML Structure Analysis\n",
    "\n",
    "To accurately extract the defined data fields, the HTML structure of Hellowork job listings was analyzed using browser developer tools. This step identifies the **HTML tags, CSS selectors, and containers** corresponding to each field.\n",
    "\n",
    "### Mapping of Data Fields to HTML Elements\n",
    "\n",
    "| Field | HTML Tag / CSS Selector | Notes |\n",
    "|-------|------------------------|-------|\n",
    "| `Job_Title` | `[data-cy=\"jobTitle\"]` (usually inside `<h1>` or `<a>`) | Main title of the job; used as the primary identifier for the job offer |\n",
    "| `Company` | `h1 a` | Name of the hiring company; displayed near the job title |\n",
    "| `Location` | `ul.tw-flex.tw-flex-wrap.tw-gap-3 li:nth-child(1)` | City/region of the job; first `<li>` under job info list |\n",
    "| `Contract` | `ul.tw-flex.tw-flex-wrap.tw-gap-3 li:nth-child(2)` | Employment type (CDI, CDD, Internship); second `<li>` under job info list |\n",
    "| `Salary` | `[data-cy=\"salary-tag-button\"]` | Optional; may be missing for some offers |\n",
    "| `Description` | `[data-truncate-text-target=\"content\"]` | Full textual job description; line breaks are removed in preprocessing |\n",
    "| `URL` | `href` attribute of the `<a>` tag linking to job offer | Direct link to job details; used for navigation and validation |\n",
    "| `Sector` | N/A (from sector list in scraping loop) | Assigned based on the sector being scraped |\n",
    "| `additional_info` | Optional; not explicitly extracted in current code | Can be added later if extra details are needed |\n",
    "\n",
    "### Notes\n",
    "- Some job postings may **omit optional fields** (e.g., `Salary` or `Contract`). Missing values are handled in ETL.  \n",
    "- All fields are **available in the loaded HTML** after page load; no dynamic JS rendering is required for the current selectors.  \n",
    "- Using this mapping, the scraping script can reliably extract each field for all job offers across sectors.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c723e0a",
   "metadata": {},
   "source": [
    "## Phase 1 – Scraping Workflow and Strategy\n",
    "\n",
    "With the HTML structure and data fields defined, the scraping workflow is designed to extract all job offers efficiently and reliably while respecting ethical standards.\n",
    "\n",
    "### 1. Workflow Overview\n",
    "\n",
    "1. **Set up Selenium WebDriver** (Chrome) with proper options:\n",
    "   - User-agent header\n",
    "   - Maximized window\n",
    "   - Optional headless mode\n",
    "2. **Access the search results page** for each sector.\n",
    "3. **Handle cookies banners** and other pop-ups to ensure uninterrupted scraping.\n",
    "4. **Iterate over each job listing on the page**:\n",
    "   - Extract job URL from the search results\n",
    "   - Open job detail page to scrape all defined data fields (`Job_Title`, `Company`, `Location`, `Contract`, `Salary`, `Description`, `URL`, `Sector`)\n",
    "   - Use **explicit waits** to ensure elements are loaded before extraction\n",
    "5. **Store extracted data** in a structured format (CSV)\n",
    "6. **Repeat for all pages** of each sector until the target of ≥ 2,000 job offers is reached\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Pagination Handling\n",
    "\n",
    "- Each search result page contains multiple job listings (typically 20–25 per page)  \n",
    "- The scraper navigates through pages by **modifying the page number parameter** in the URL (`&p=page_number`)  \n",
    "- Iteration continues until:\n",
    "  - Enough job offers are collected (≥ 2,000)  \n",
    "  - No more pages are available or the page contains no job listings\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Error Handling\n",
    "\n",
    "- **Missing fields**: optional fields (Salary, Contract) are stored as `\"N/A\"` or `None`  \n",
    "- **Timeouts / page load issues**: use Selenium **explicit waits** and retry logic for failed elements  \n",
    "- **Unexpected page structure**: log the issue and continue with remaining jobs  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Politeness and Ethics\n",
    "\n",
    "- Add a **delay of 0.5–3 seconds** between requests and job detail visits to avoid overloading the server  \n",
    "- Respect the website’s **robots.txt** and terms of use  \n",
    "- Scraping is performed **only for academic purposes**, not for commercial use  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data Storage\n",
    "\n",
    "- Extracted data is **saved incrementally** to `hellowork_progress.csv` to prevent loss in case of interruption  \n",
    "- Final dataset saved as `hellowork_final_sectors_data.csv`  \n",
    "- CSV encoding: UTF-8 to preserve special characters  \n",
    "- Data includes all defined fields and sector information, ready for **Phase 2 – ETL and Data Cleaning**\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Summary\n",
    "\n",
    "This workflow ensures:\n",
    "- Reliable extraction of ≥ 2,000 job offers across multiple sectors  \n",
    "- Structured and consistent dataset with all required fields  \n",
    "- Ethical, reproducible, and robust scraping process using Selenium  \n",
    "- Preparedness for subsequent **ETL, ML, and interactive dashboard** phases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50362de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException , NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97795968",
   "metadata": {},
   "source": [
    "## Libraries Used in Phase 1 – Scraping\n",
    "\n",
    "- `time`  \n",
    "  Provides functions to **pause execution** (e.g., `sleep`) between requests to avoid overloading the server.\n",
    "\n",
    "- `pandas` (`pd`)  \n",
    "  Used for **data manipulation and storage**. Allows saving scraped job offers to CSV (`.to_csv`) and performing ETL in later phases.\n",
    "\n",
    "- `selenium`  \n",
    "  Automates **browser interactions** for scraping dynamic websites. Main components used:  \n",
    "  - `webdriver` – controls the browser (Chrome in this case)  \n",
    "  - `Service` – manages the ChromeDriver service  \n",
    "  - `By` – locates HTML elements by ID, CSS selector, XPath, etc.  \n",
    "  - `WebDriverWait` – explicit waits until elements are present or clickable  \n",
    "  - `expected_conditions` (`EC`) – conditions for waits, e.g., element visibility or clickability  \n",
    "  - `TimeoutException` – handles cases when a wait exceeds the maximum time  \n",
    "\n",
    "- `webdriver_manager.chrome` (`ChromeDriverManager`)  \n",
    "  Automatically **downloads and manages the correct ChromeDriver version**, simplifying Selenium setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6808e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cookies banner found.\n",
      "\n",
      "--- START SECTOR: Agriculture • Pêche (ID: Agri_peche) ---\n",
      "Page 1 | Sector URL: https://www.hellowork.com/fr-fr/emploi/recherche.html?k=job+%C3%A9tudiant&st=relevance&s=Agri_peche&p=1\n",
      "Scraping terminé. Fichier CSV : hellowork_final_sectors_data.csv | Total jobs : 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchWindowException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Sector URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msector_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msector_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:452\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03mThe method does not return until the page is fully loaded (i.e. the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \u001b[33;03m    `driver.get(\"https://example.com\")`\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:432\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mNoSuchWindowException\u001b[39m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=143.0.7499.41)\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x5d0d13\n\t0x5d0d54\n\t0x3be6dd\n\t0x39ca8d\n\t0x431ffb\n\t0x44d44c\n\t0x42b2e6\n\t0x3fd321\n\t0x3fe1d4\n\t0x824d54\n\t0x82030b\n\t0x83cbea\n\t0x5eac18\n\t0x5f2c1d\n\t0x5d9018\n\t0x5d91d9\n\t0x5c3568\n\t0x76015d49\n\t0x770cd5db\n\t0x770cd561\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:849\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    848\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    149\u001b[39m         driver.quit()\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    147\u001b[39m df.to_csv(final_filename, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8-sig\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraping terminé. Fichier CSV : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Total jobs : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:218\u001b[39m, in \u001b[36mChromiumDriver.quit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:156\u001b[39m, in \u001b[36mService.stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process.poll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_remote_shutdown_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:142\u001b[39m, in \u001b[36mService.send_remote_shutdown_command\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m30\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    144\u001b[39m     sleep(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:132\u001b[39m, in \u001b[36mService.is_connectable\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    127\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Establish a socket connection to determine if the service is accessible.\u001b[39;00m\n\u001b[32m    128\u001b[39m \n\u001b[32m    129\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m        True if the service is connectable on the configured port, False otherwise.\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\utils.py:121\u001b[39m, in \u001b[36mis_connectable\u001b[39m\u001b[34m(port, host)\u001b[39m\n\u001b[32m    119\u001b[39m socket_ = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     socket_ = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     result = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\faleg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:856\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    855\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m         \u001b[43mexceptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[32m    857\u001b[39m     exceptions.append(exc)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "BASE_SEARCH_URL = \"https://www.hellowork.com/fr-fr/emploi/recherche.html?k=job+%C3%A9tudiant&st=relevance\"\n",
    "MAX_PAGES_PER_SECTOR = 10\n",
    "\n",
    "SECTORS_LIST = [\n",
    "    {\"id\": \"Agri_peche\", \"name\": \"Agriculture • Pêche\"},\n",
    "    {\"id\": \"BTP\", \"name\": \"BTP\"},\n",
    "    {\"id\": \"Banq_assur_finan\", \"name\": \"Banque • Assurance • Finance\"},\n",
    "    {\"id\": \"Distrib_commerce\", \"name\": \"Distribution • Commerce de gros\"},\n",
    "    {\"id\": \"Enseign_forma\", \"name\": \"Enseignement • Formation\"},\n",
    "    {\"id\": \"Immo\", \"name\": \"Immobilier\"},\n",
    "    {\"id\": \"Ind_agro\", \"name\": \"Industrie Agro • alimentaire\"},\n",
    "    {\"id\": \"Ind_auto_meca_nav\", \"name\": \"Industrie Auto • Meca • Navale\"},\n",
    "    {\"id\": \"Ind_aero\", \"name\": \"Industrie Aéronautique • Aérospatial\"},\n",
    "    {\"id\": \"Ind_manufact\", \"name\": \"Industrie Manufacturière\"},\n",
    "    {\"id\": \"Ind_pharma_bio_chim\", \"name\": \"Industrie Pharmaceutique • Biotechn. • Chimie\"},\n",
    "    {\"id\": \"Ind_petro\", \"name\": \"Industrie Pétrolière • Pétrochimie\"},\n",
    "    {\"id\": \"Ind_hightech_telecom\", \"name\": \"Industrie high • tech • Telecom\"},\n",
    "    {\"id\": \"Media_internet_com\", \"name\": \"Média • Internet • Communication\"},\n",
    "    {\"id\": \"Resto\", \"name\": \"Restauration\"},\n",
    "    {\"id\": \"Sante_social\", \"name\": \"Santé • Social • Association\"},\n",
    "    {\"id\": \"Energie_envir\", \"name\": \"Secteur Energie • Environnement\"},\n",
    "    {\"id\": \"Inform_SSII\", \"name\": \"Secteur informatique • ESN\"},\n",
    "    {\"id\": \"Serv_public_autre\", \"name\": \"Service public autres\"},\n",
    "    {\"id\": \"Serv_public_etat\", \"name\": \"Service public d'état\"},\n",
    "    {\"id\": \"Serv_public_collec_terri\", \"name\": \"Service public des collectivités territoriales\"},\n",
    "    {\"id\": \"Serv_public_hosp\", \"name\": \"Service public hospitalier\"},\n",
    "    {\"id\": \"Serv_entreprise\", \"name\": \"Services aux Entreprises\"},\n",
    "    {\"id\": \"Serv_pers_part\", \"name\": \"Services aux Personnes • Particuliers\"},\n",
    "    {\"id\": \"Tourism_hotel_loisir\", \"name\": \"Tourisme • Hôtellerie • Loisirs\"},\n",
    "    {\"id\": \"Transport_logist\", \"name\": \"Transport • Logistique\"}\n",
    "]\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Décommenter pour exécution en arrière-plan\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def handle_cookies(driver):\n",
    "    \"\"\"Ferme la bannière de cookies si présente.\"\"\"\n",
    "    try:\n",
    "        cookie_btn = WebDriverWait(driver, 4).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"hw-cc-notice-continue-without-accepting-btn\"))\n",
    "        )\n",
    "        cookie_btn.click()\n",
    "        print(\"Cookies handled.\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"No cookies banner found.\")\n",
    "\n",
    "def scrape_job_details(driver, url, sector_name):\n",
    "    \"\"\"Scrape les détails d'une offre spécifique.\"\"\"\n",
    "    driver.get(url)\n",
    "    data = {\n",
    "        \"Sector\": sector_name,\n",
    "        \"Job_Title\": \"N/A\",\n",
    "        \"Company\": \"N/A\",\n",
    "        \"Location\": \"N/A\",\n",
    "        \"Contract\": \"N/A\",\n",
    "        \"Salary\": \"N/A\",\n",
    "        \"Description\": \"N/A\",\n",
    "        \"URL\": url\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "\n",
    "        # 1. Job Title\n",
    "        try:\n",
    "            data[\"Job_Title\"] = driver.find_element(By.CSS_SELECTOR, '[data-cy=\"jobTitle\"]').text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 2. Company\n",
    "        try:\n",
    "            data[\"Company\"] = driver.find_element(By.CSS_SELECTOR, 'h1 a').text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 3. Location & Contract\n",
    "        try:\n",
    "            tags = driver.find_elements(By.CSS_SELECTOR, 'ul.tw-flex.tw-flex-wrap.tw-gap-3 li')\n",
    "            if len(tags) > 0: data[\"Location\"] = tags[0].text.strip()\n",
    "            if len(tags) > 1: data[\"Contract\"] = tags[1].text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 4. Salary\n",
    "        try:\n",
    "            data[\"Salary\"] = driver.find_element(By.CSS_SELECTOR, '[data-cy=\"salary-tag-button\"]').text.strip()\n",
    "        except: pass\n",
    "\n",
    "        # 5. Description\n",
    "        try:\n",
    "            desc = driver.find_element(By.CSS_SELECTOR, '[data-truncate-text-target=\"content\"]').text\n",
    "            data[\"Description\"] = desc.replace(\"\\n\", \" \").strip()\n",
    "        except: pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details for {url}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    all_results = []\n",
    "\n",
    "    try:\n",
    "        driver.get(BASE_SEARCH_URL)\n",
    "        handle_cookies(driver)\n",
    "\n",
    "        for sector in SECTORS_LIST:\n",
    "            s_name = sector['name']\n",
    "            s_id = sector['id']\n",
    "            \n",
    "            print(f\"\\n--- START SECTOR: {s_name} (ID: {s_id}) ---\")\n",
    "\n",
    "            for page in range(1, MAX_PAGES_PER_SECTOR + 1):\n",
    "                sector_url = f\"{BASE_SEARCH_URL}&s={s_id}&p={page}\"\n",
    "                print(f\"Page {page} | Sector URL: {sector_url}\")\n",
    "                driver.get(sector_url)\n",
    "\n",
    "                try:\n",
    "                    WebDriverWait(driver, 6).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'a[data-cy=\"offerTitle\"]'))\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    print(f\"No jobs on page {page}, skipping sector.\")\n",
    "                    break\n",
    "\n",
    "                offer_elems = driver.find_elements(By.CSS_SELECTOR, 'a[data-cy=\"offerTitle\"]')\n",
    "                urls_to_visit = list(set([elem.get_attribute(\"href\") for elem in offer_elems]))\n",
    "                print(f\"Found {len(urls_to_visit)} jobs.\")\n",
    "\n",
    "                for url in urls_to_visit:\n",
    "                    job_data = scrape_job_details(driver, url, s_name)\n",
    "                    all_results.append(job_data)\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "            # Sauvegarde intermédiaire\n",
    "            pd.DataFrame(all_results).to_csv(\"hellowork_progress.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    finally:\n",
    "        # Sauvegarde finale\n",
    "        df = pd.DataFrame(all_results)\n",
    "        final_filename = \"hellowork_final_sectors_data.csv\"\n",
    "        df.to_csv(final_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Scraping terminé. Fichier CSV : {final_filename} | Total jobs : {len(all_results)}\")\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
